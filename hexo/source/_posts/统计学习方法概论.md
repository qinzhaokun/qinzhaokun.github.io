---
title: 统计学习方法概论
date: 2017-07-26
tags: 机器学习
categories: 机器学习
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


## 概括

统计学习是关于计算机基础数据构建概率统计模型的并运模型对数据进行预测与分析的一门学科。

### 对象

**统计学习**的对象是数据，从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析和预测当中。数据包括数字，文字，图像，视频等

### 目的

对数据进行预测和分析，给人们获得新知识，给人们带来新发现

### 前提

**统计学习关于数据的假设是同类数据具有一定的统计规律性**，这是统计学习的前提。

### 方法

统计学习由*监督学习*，*非监督学习*，*半监督学习*和*强化学习*等组成。其中**监督学习**是主要讨论的。它由三要素构成，模型的假设空间（**模型（model）**），模型选择的准则（**策略（strategy）**）和模型学习的算法（**算法（algorithm）**）

## 监督学习

学习出一个模型，使得模型对任意给定的输入，对应相应的输出做出一个很好的预测

### 基本概念

#### 输入空间，特征空间和输出空间

输入和输出的所有可能取值的集合分别称为*输入空间*和*输出空间*，它们可以是有限元素的集合，也可以是整个欧式空间，通常输出空间远小于输入空间。每一个具体的输入时一个实例（instance），通常有*特征向量*表示，所有特征向量称为特征空间

#### 联合概率分布

监督学习假设输入和输出的随机变量X和Y遵循联合概率分布P(X,Y)，这是关于数据的基本假设，训练数据和测试数据都是被看做依照联合概率分布P(X,Y)堵路同分布产生的。

#### 假设空间

监督学习的目的就是在于**学习一个由输入到输出的映射**，而输入空间到输出空间的映射的集合，就是*假设空间*。这种映射关系可以**条件概率分布**P(Y|X)或**决策函数**Y=f(X)决定

## 统计学习的三要素

方法=模型+策略+算法

### 模型

我们需要学习什么样的模型，就是要学习什么样的条件概率分布或决策函数。*假设空间*包含所有可能的条件概率分布或决策函数，假设空间一般是无穷多的。一般的，由条件概率分布表示的为概率模型，而由决策函数表示的为概率模型

### 策略

学习的准则是什么？如何从假设空间中选取最优模型。因此，我们引入了损失函数和风险函数。损失函数是用来度量预测错误的程度，例如输入为X，决策函数为f，输出为f(X), 损失函数记为L(Y,f(X)), 常用的损失函数如下：

1. 0-1损失函数

2. 平方损失函数

3. 绝对损失函数

损失函数越小，模型就越好。模型的输入和输出是随机变量，遵循联合分布P(X,Y),损失函数的期望是*风险函数*或期望损失，又叫做期望风险。我们学习的目的就是要*选择期望风险最小的模型*。计算期望风险需要知道联合分布，但是这正是我们需要学习的。因此无法直接计算期望风险。由此引入经验风险，它是建立在训练数据的平均损失。根据大数定律，当训练样本趋于无穷大时，经验风险等于期望风险。

但是，样本根本不可能无穷大，是非常有限的，甚至样本有可能是错误的。**因此需要对经验风险进行矫正**。引入两个概念，*经验风险最小化*和*结构风险最小化*
经验风险认为经验风险越小，模型就越优，现实中被广泛采用。*极大似然估计*就是一个例子。当样本足够大时，效果不错，但是当样本容量很小时，*经验风险最小化*未必最好，会产生**过拟合**的现象。

*结构风险最小化*是为了防止过拟合而提出的策略，它通过在损失函数后加上正则化项或罚项，从而来权衡经验风险和模型复杂度。最大后验概率就是一个例子。

### 算法

寻找模型的具体计算方法。

## 交叉验证

它是一种模型选择的方法，当数据不充足的情况下，对数据反复进行切分，组合成不同训练集和测试集，反复进行训练测试和模型选择，具体可分为：

+ 简单交叉验证

+ S折交叉验证

+ 留-交叉验证

## 生成模型和判别模型

在监督学习中，学习一个模型，一般形式有决策函数和条件概率分布

### 生成模型（generative model）

该模型学习的是联合概率分布P(X,Y),然后求出条件概率分布P(Y|X)作为预测模型，表示了给定输入X产生输出Y的关系，具体模型有：朴素贝叶斯发和隐马尔科夫模型。优点：可还原出联合概率P(X,Y)，收敛速度快，存在隐变量时，仍然可使用

### 判别模型（discriminative model）

该模型直接学习决策函数f(X)或条件概率分布P(Y|X)作为预测模型，关心对给定的输入X，应预测什么样的输出Y。模型包括：k邻近法，感知机，决策树，逻辑回归，支持向量机，条件随机场等。优点：准确率高，可对数据进行抽象，简化学习。

## 分类问题

分为学习和分类两个过程，学习过程中，根据已知的训练数据集，利用有效的学习方法学习一个分类器，在分类过程中，利用学习的分类器对新的输入实例进行分类。有两个评价指标，精确率和召回率;

TP -- 将正类预测为正类

FN -- 将正类预测为负类

FP -- 将负类预测为正类

TN -- 将负类预测为负类

精确率：P = TP/(TP+FP);

召回率： R = TP/(TP+FN)；

一个文本检索的例子，总共有100条文本，其中有60条包含关键字Apple，40条无该关键字，如果一个检索器，检索出50条记录，其中40条有关键字。那么精确率表示检索出正确的比率：40/50=80%， 召回率表示有多少正确的被检索出来了，40/60=66%

## 回归问题
用于预测输入变量和输出变量之间的关系，等价于求变量之间的映射函数，函数拟合的过程

> 若我们欲预测的是**离散值**，例如"好瓜""坏瓜"，此类学习任务称为 "分类"; 若欲预测的是**连续值**，例如西瓜的成熟度0.95 ,0.37,此类学习任务称为"回归"。
